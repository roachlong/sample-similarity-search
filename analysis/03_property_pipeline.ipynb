{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa88932",
   "metadata": {},
   "source": [
    "## AI Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a219950-7721-4e1a-8a11-86e451c64447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jleelong/workspace/sample-similarity-search/analysis\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "\n",
    "source_directory = '../data'\n",
    "file_directory = '../data/sample'\n",
    "\n",
    "# Set pandas display options to show all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4391180b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "199da33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_na_attributes(data):\n",
    "    \"\"\"\n",
    "    Recursively removes attributes with NA values from a JSON-like dictionary.\n",
    "\n",
    "    Args:\n",
    "        data (dict): A dictionary representing JSON data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A new dictionary with NA attributes removed.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        return {\n",
    "            k: remove_na_attributes(v) for k, v in data.items() if v is not None and v != \"NA\"\n",
    "        }\n",
    "    elif isinstance(data, list):\n",
    "        return [remove_na_attributes(item) for item in data]\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def load_json_files_to_dataframe(file_dir):\n",
    "    \"\"\"\n",
    "    Loads multiple JSON files from a directory into a single Pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_dir (str): The directory containing the JSON files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the combined data from all JSON files.\n",
    "                      Returns an empty DataFrame if no files are found or if an error occurs.\n",
    "    \"\"\"\n",
    "    all_files = os.listdir(file_dir)\n",
    "    json_files = [f for f in all_files if f.endswith('.json')]\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"No JSON files found in directory: {file_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_data = []\n",
    "    for file_name in json_files:\n",
    "        file_path = os.path.join(file_dir, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                all_data.append(data)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON in file: {file_path}\")\n",
    "        except FileNotFoundError:\n",
    "             print(f\"File not found: {file_path}\")\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if not isinstance(all_data, list):\n",
    "         return pd.DataFrame()\n",
    "\n",
    "    # all_data = remove_na_attributes(all_data)\n",
    "    return pd.concat([pd.json_normalize(data) for data in all_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b79232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_building_class(value):\n",
    "    if pd.notna(value):\n",
    "        if value.isdigit():\n",
    "            return int(value)\n",
    "        else:\n",
    "            try:\n",
    "                value = value.replace('+', '.5')\n",
    "                return round(float(value))\n",
    "            except ValueError:\n",
    "                return None\n",
    "\n",
    "def fill_building_class(tot_asmt, bld_cls, avg_bld_class):\n",
    "    if (pd.isna(bld_cls)):\n",
    "        valid_indices = avg_bld_class.index[avg_bld_class.index <= tot_asmt]\n",
    "        if not valid_indices.empty:\n",
    "          return avg_bld_class[valid_indices.max()]\n",
    "        valid_indices = avg_bld_class.index[avg_bld_class.index > tot_asmt]\n",
    "        if not valid_indices.empty:\n",
    "          return avg_bld_class[valid_indices.min()]\n",
    "        else:\n",
    "          return 0\n",
    "    else:\n",
    "        return bld_cls\n",
    "\n",
    "def fill_square_feet(bld_asmt, sq_ft, avg_feet):\n",
    "    if (pd.isna(sq_ft)):\n",
    "        valid_indices = avg_feet.index[avg_feet.index <= bld_asmt]\n",
    "        if not valid_indices.empty:\n",
    "          return avg_feet[valid_indices.max()]\n",
    "        valid_indices = avg_feet.index[avg_feet.index > bld_asmt]\n",
    "        if not valid_indices.empty:\n",
    "          return avg_feet[valid_indices.min()]\n",
    "        else:\n",
    "          return 0\n",
    "    else:\n",
    "        return sq_ft\n",
    "\n",
    "def fill_no_of_dwellings(bld_cls, sq_ft, no_dwell, avg_dwellings):\n",
    "    if np.isnan(no_dwell):\n",
    "        result = 0\n",
    "        segment = avg_dwellings[(avg_dwellings['Building_Class'] == bld_cls)]     \n",
    "        if not segment.empty:\n",
    "            filtered = segment[segment['Sq_Ft'] <= sq_ft]\n",
    "            if not filtered.empty:\n",
    "                result = filtered.loc[filtered.idxmax()['Sq_Ft']]['No_Of_Dwellings']\n",
    "            else:\n",
    "                filtered = segment[segment['Sq_Ft'] > sq_ft]\n",
    "                if not filtered.empty:\n",
    "                    result = filtered.loc[filtered.idxmin()['Sq_Ft']]['No_Of_Dwellings']\n",
    "        return round(result)\n",
    "    else:\n",
    "        return no_dwell\n",
    "\n",
    "def fill_year_built(bld_cls, tax_rate, sq_ft, yr_built, avg_year, avg_year_class):\n",
    "    if np.isnan(yr_built):\n",
    "        result = 0\n",
    "        segment = avg_year[(avg_year['Building_Class'] == bld_cls) &\n",
    "                           (avg_year['TaxRate'] == tax_rate)]\n",
    "        if segment.empty:\n",
    "            segment = avg_year_class[avg_year_class['Building_Class'] == bld_cls]        \n",
    "        if not segment.empty:\n",
    "            filtered = segment[segment['Sq_Ft'] <= sq_ft]\n",
    "            if not filtered.empty:\n",
    "                result = filtered.loc[filtered.idxmax()['Sq_Ft']]['Yr_Built']\n",
    "            else:\n",
    "                filtered = segment[segment['Sq_Ft'] > sq_ft]\n",
    "                if not filtered.empty:\n",
    "                    result = filtered.loc[filtered.idxmin()['Sq_Ft']]['Yr_Built']\n",
    "        return round(result)\n",
    "    else:\n",
    "        return yr_built\n",
    "\n",
    "def fill_recorded_taxes(calc_tax, sq_ft, rec_tax, avg_taxes):\n",
    "    if np.isnan(rec_tax):\n",
    "        result = calc_tax\n",
    "        segment = avg_taxes[avg_taxes['Calculated_Taxes'] == calc_tax]\n",
    "        if segment.empty:\n",
    "            filtered = avg_taxes[avg_taxes['Calculated_Taxes'] <= calc_tax]\n",
    "            if not filtered.empty:\n",
    "                calc_tax = filtered.loc[filtered.idxmax()['Calculated_Taxes']]['Calculated_Taxes']\n",
    "                segment = avg_taxes[avg_taxes['Calculated_Taxes'] == calc_tax]\n",
    "            else:\n",
    "                filtered = avg_taxes[avg_taxes['Calculated_Taxes'] > calc_tax]\n",
    "                if not filtered.empty:\n",
    "                    calc_tax = filtered.loc[filtered.idxmin()['Calculated_Taxes']]['Calculated_Taxes']\n",
    "                    segment = avg_taxes[avg_taxes['Calculated_Taxes'] == calc_tax]    \n",
    "        if not segment.empty:\n",
    "            filtered = segment[segment['Sq_Ft'] <= sq_ft]\n",
    "            if not filtered.empty:\n",
    "                result = filtered.loc[filtered.idxmax()['Sq_Ft']]['Recorded_Taxes']\n",
    "            else:\n",
    "                filtered = segment[segment['Sq_Ft'] > sq_ft]\n",
    "                if not filtered.empty:\n",
    "                    result = filtered.loc[filtered.idxmin()['Sq_Ft']]['Recorded_Taxes']\n",
    "        return result\n",
    "    else:\n",
    "        return rec_tax\n",
    "\n",
    "def fill_corporate_owned(absentee, no_dwell, bld_cls, corp_owned,\n",
    "                         avg_corp_owned, avg_corp_owned_dwell, avg_corp_owned_absent):\n",
    "    if pd.isnull(corp_owned):\n",
    "        result = 0\n",
    "        segment = avg_corp_owned[(avg_corp_owned['Absentee'] == absentee) &\n",
    "                                 (avg_corp_owned['No_Of_Dwellings'] == no_dwell) &\n",
    "                                 (avg_corp_owned['Building_Class'] == bld_cls)]\n",
    "        if segment.empty:\n",
    "            segment = avg_corp_owned_dwell[(avg_corp_owned_dwell['Absentee'] == absentee) &\n",
    "                                           (avg_corp_owned_dwell['No_Of_Dwellings'] == no_dwell)]\n",
    "        if segment.empty:\n",
    "            segment = avg_corp_owned_absent[(avg_corp_owned_absent['Absentee'] == absentee)]  \n",
    "        if not segment.empty:\n",
    "            result = segment.mean()['Corporate_Owned']\n",
    "        return round(result)\n",
    "    else:\n",
    "        return corp_owned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d461ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_dir, model_name):\n",
    "    df = load_json_files_to_dataframe(file_dir)\n",
    "    df = df.dropna(subset=['countyData.Sale_Price'])\n",
    "    \n",
    "    columns_to_keep = ['location.lat', 'location.lng', 'countyData.No_Of_Dwellings',\n",
    "                       'countyData.Corporate_Owned', 'countyData.Absentee', 'countyData.NU_Code',\n",
    "                       'countyData.updated_at', 'countyData.TotalUnits', 'countyData.Sq_Ft',\n",
    "                       'countyData.Property_Class', 'countyData.Building_Class', 'countyData.Yr_Built',\n",
    "                       'countyData.Sale_Date', 'countyData.TaxRate', 'countyData.TaxRatio',\n",
    "                       'countyData.RateYear', 'countyData.Recorded_Taxes', 'countyData.Calculated_Taxes',\n",
    "                       'countyData.Calculated_Taxes_Year', 'countyData.Year_1', 'countyData.Land_Assmnt_1',\n",
    "                       'countyData.Building_Assmnt_1', 'countyData.Total_Assmnt_1', 'countyData.Sale_Price']\n",
    "    df = df[columns_to_keep]\n",
    "    df = df.rename(columns={'location.lat': 'lat', 'location.lng': 'lng',\n",
    "                            'countyData.No_Of_Dwellings': 'No_Of_Dwellings',\n",
    "                            'countyData.Corporate_Owned': 'Corporate_Owned',\n",
    "                            'countyData.Absentee': 'Absentee', 'countyData.NU_Code': 'NU_Code',\n",
    "                            'countyData.updated_at': 'updated_at', 'countyData.TotalUnits': 'TotalUnits',\n",
    "                            'countyData.Sq_Ft': 'Sq_Ft', 'countyData.Property_Class': 'Property_Class',\n",
    "                            'countyData.Building_Class': 'Building_Class', 'countyData.Yr_Built': 'Yr_Built',\n",
    "                            'countyData.Sale_Date': 'Sale_Date', 'countyData.TaxRate': 'TaxRate',\n",
    "                            'countyData.TaxRatio': 'TaxRatio', 'countyData.RateYear': 'RateYear',\n",
    "                            'countyData.Recorded_Taxes': 'Recorded_Taxes',\n",
    "                            'countyData.Calculated_Taxes': 'Calculated_Taxes',\n",
    "                            'countyData.Calculated_Taxes_Year': 'Calculated_Taxes_Year',\n",
    "                            'countyData.Year_1': 'Year_1', 'countyData.Land_Assmnt_1': 'Land_Assmnt_1',\n",
    "                            'countyData.Building_Assmnt_1': 'Building_Assmnt_1',\n",
    "                            'countyData.Total_Assmnt_1': 'Total_Assmnt_1', 'countyData.Sale_Price': 'Sale_Price'})\n",
    "    \n",
    "    df = df.dropna(subset=['Land_Assmnt_1', 'Building_Assmnt_1', 'Total_Assmnt_1'])\n",
    "    df = df.dropna(subset=['lat', 'lng'])\n",
    "    \n",
    "#     df['County'] = df['id'].apply(lambda x: int(x[:2].lstrip(\"0\")))\n",
    "#     df['Municipality'] = df['id'].apply(lambda x: int(x[2:4].lstrip(\"0\")))\n",
    "#     df = df.drop('id', axis=1)\n",
    "    \n",
    "    df['NU_Code'] = df['NU_Code'].apply(lambda x: int(x) if pd.notna(x) and x.isdigit()\n",
    "                                        else -1 if pd.notna(x) else 99)\n",
    "    df = df[df['NU_Code'] == 99]\n",
    "    df = df.drop('NU_Code', axis=1)\n",
    "    \n",
    "    df['updated_at'] = pd.to_datetime(df['updated_at']).values.astype('datetime64[ms]').astype(int)\n",
    "    df['updated_at'] = df['updated_at'].fillna(0)\n",
    "    \n",
    "    df['Property_Class'] = df['Property_Class'].map({\n",
    "        '1': 1, '2': 2, '3A': 3, '3B': 4, '4A': 5, '4B': 6, '4C': 7, '5A': 8, '5B': 9, '6A': 10,\n",
    "        '6B': 11, '6C': 12, '15A': 13, '15B': 14, '15C': 15, '15D': 16, '15E': 17, '15F': 18\n",
    "    })\n",
    "    df = df[df['Property_Class'] == 2]\n",
    "    df = df.drop(['Property_Class', 'TotalUnits'], axis=1)\n",
    "    \n",
    "    df['Sale_Date'] = pd.to_datetime(df['Sale_Date'])\n",
    "    df['Sale_Month'] = df['Sale_Date'].dt.month.astype(pd.Int64Dtype())\n",
    "    df['Sale_Year'] = df['Sale_Date'].dt.year.astype(pd.Int64Dtype())\n",
    "    df = df.drop('Sale_Date', axis=1)\n",
    "    df = df.dropna(subset=['Sale_Month', 'Sale_Year'])\n",
    "\n",
    "    df['TaxRate'] = df['TaxRate'].astype(float)\n",
    "    df['TaxRatio'] = df['TaxRatio'].astype(float)\n",
    "    df['RateYear'] = df['RateYear'].astype(int)\n",
    "\n",
    "    df['Building_Class'] = df['Building_Class'].apply(lambda x: convert_building_class(x)).astype(pd.Int64Dtype())\n",
    "    update_mapping = pd.DataFrame({\n",
    "        'key':[\n",
    "            26, 27, 28, 29, 30,\n",
    "            33, 34, 35, 36, 37, 38, 39,\n",
    "            43, 44, 45, 46, 47, 48, 49,\n",
    "            50, 51, 52, 53, 54, 55\n",
    "        ], 'value':[\n",
    "            12, 13, 14, 16, 18,\n",
    "            13, 14, 15, 16, 17, 18, 19,\n",
    "            13, 14, 15, 16, 17, 18, 19,\n",
    "            12, 13, 15, 18, 19, 20\n",
    "        ]\n",
    "    })\n",
    "    df['Building_Class'] = df['Building_Class'].replace(dict(zip(update_mapping['key'], update_mapping['value'])))\n",
    "    avg_bld_class = df[df['Building_Class'].notna()].groupby('Total_Assmnt_1').mean()['Building_Class']\n",
    "    df['Building_Class'] = df.apply(lambda x: fill_building_class(x['Total_Assmnt_1'],\n",
    "                                                                  x['Building_Class'], avg_bld_class), axis=1)\n",
    "    df['Building_Class'] = df['Building_Class'].astype(pd.Float64Dtype()).round().astype(pd.Int64Dtype())\n",
    "        \n",
    "    avg_feet = df[df['Sq_Ft'].notna()].groupby('Building_Assmnt_1').mean()['Sq_Ft']\n",
    "    df['Sq_Ft'] = df.apply(lambda x: fill_square_feet(x['Building_Assmnt_1'], x['Sq_Ft'], avg_feet), axis=1)\n",
    "\n",
    "    avg_dwellings = df[df['No_Of_Dwellings'].notna()].groupby(['Building_Class', 'Sq_Ft']).agg(\n",
    "        {'No_Of_Dwellings': 'mean'}).reset_index()\n",
    "    df['No_Of_Dwellings'] = df.apply(lambda x: fill_no_of_dwellings(x['Building_Class'],\n",
    "                                                                    x['Sq_Ft'], x['No_Of_Dwellings'],\n",
    "                                                                    avg_dwellings), axis=1)\n",
    "    df['No_Of_Dwellings'] = df['No_Of_Dwellings'].astype(int)\n",
    "\n",
    "    avg_year = df[df['Yr_Built'].notna()].groupby(['Building_Class', 'TaxRate', 'Sq_Ft']).agg(\n",
    "        {'Yr_Built': 'mean'}).reset_index()\n",
    "    avg_year_class = df[df['Yr_Built'].notna()].groupby(['Building_Class', 'Sq_Ft']).agg(\n",
    "        {'Yr_Built': 'mean'}).reset_index()\n",
    "    df['Yr_Built'] = df.apply(lambda x: fill_year_built(x['Building_Class'], x['TaxRate'],\n",
    "                                                        x['Sq_Ft'], x['Yr_Built'],\n",
    "                                                        avg_year, avg_year_class), axis=1)\n",
    "    df['Yr_Built'] = df['Yr_Built'].astype(int)\n",
    "\n",
    "    avg_taxes = df[df['Recorded_Taxes'].notna()].groupby(['Calculated_Taxes', 'Sq_Ft']).agg(\n",
    "        {'Recorded_Taxes': 'mean'}).reset_index()\n",
    "    df['Recorded_Taxes'] = df.apply(lambda x: fill_recorded_taxes(x['Calculated_Taxes'], x['Sq_Ft'],\n",
    "                                                                  x['Recorded_Taxes'], avg_taxes), axis=1)\n",
    "    \n",
    "    df = df.drop(['Calculated_Taxes', 'Calculated_Taxes_Year'], axis=1)\n",
    "\n",
    "    avg_corp_owned = df[df['Corporate_Owned'].notna()].groupby(['Absentee', 'No_Of_Dwellings',\n",
    "                                                                'Building_Class']).agg(\n",
    "        {'Corporate_Owned': 'mean'}).reset_index()\n",
    "    avg_corp_owned_dwell = df[df['Corporate_Owned'].notna()].groupby(['Absentee', 'No_Of_Dwellings']).agg(\n",
    "        {'Corporate_Owned': 'mean'}).reset_index()\n",
    "    avg_corp_owned_absent = df[df['Corporate_Owned'].notna()].groupby(['Absentee']).agg(\n",
    "        {'Corporate_Owned': 'mean'}).reset_index()\n",
    "    df['Corporate_Owned'] = df.apply(lambda x: fill_corporate_owned(x['Absentee'], x['No_Of_Dwellings'],\n",
    "                                                                    x['Building_Class'], x['Corporate_Owned'],\n",
    "                                                                    avg_corp_owned, avg_corp_owned_dwell,\n",
    "                                                                    avg_corp_owned_absent), axis=1)\n",
    "\n",
    "#     df['County'] = df['County'].map({\n",
    "#         1: 'Atlantic',\n",
    "#         2: 'Bergen',\n",
    "#         3: 'Burlignton',\n",
    "#         4: 'Camden',\n",
    "#         5: 'Cape May',\n",
    "#         6: 'Cumberland',\n",
    "#         7: 'Essex',\n",
    "#         8: 'Gloucester',\n",
    "#         9: 'Hudson',\n",
    "#         10: 'Hunterdon',\n",
    "#         11: 'Mercer',\n",
    "#         12: 'Middlesex',\n",
    "#         13: 'Monmouth',\n",
    "#         14: 'Morris',\n",
    "#         15: 'Ocean',\n",
    "#         16: 'Passaic',\n",
    "#         17: 'Salem',\n",
    "#         18: 'Somerset',\n",
    "#         19: 'Sussex',\n",
    "#         20: 'Union',\n",
    "#         21: 'Warren'\n",
    "#     })\n",
    "#     dummies = pd.get_dummies(df['County'])\n",
    "#     df = df.drop('County', axis=1)\n",
    "#     # in case sample data is missing a county, we'll add\n",
    "#     # the columns manually and then overwrite the values\n",
    "#     # skip Atlantic because we want to drop one column\n",
    "#     counties = ['Bergen', 'Burlignton', 'Camden', 'Cape May',\n",
    "#                 'Cumberland', 'Essex', 'Gloucester', 'Hudson',\n",
    "#                 'Hunterdon', 'Mercer', 'Middlesex', 'Monmouth',\n",
    "#                 'Morris', 'Ocean', 'Passaic', 'Salem', 'Somerset',\n",
    "#                 'Sussex', 'Union', 'Warren']\n",
    "#     for county in counties:\n",
    "#         if county in dummies.columns:\n",
    "#             df[county] = dummies[county]\n",
    "#         else:\n",
    "#             df[county] = False\n",
    "    \n",
    "    df['Year_1'] = df['Year_1'].astype(int)\n",
    "    df.to_pickle(os.path.join(file_dir, 'processed', model_name + \".pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5828e293",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89043e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_fit(file_dir, model_name, X_train, X_val, y_train, y_val):\n",
    "    file_path = os.path.join(file_dir, 'scaler', model_name + \".save\")\n",
    "    if os.path.exists(file_path):\n",
    "        scaler = joblib.load(file_path)\n",
    "        X_train = scaler.transform(X_train)\n",
    "    else:\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        joblib.dump(scaler, file_path)\n",
    "\n",
    "    X_val = scaler.transform(X_val)\n",
    "#     X_test = scaler.transform(X_test)\n",
    "    \n",
    "    file_path = os.path.join(file_dir, 'model', model_name + \".h5\")\n",
    "    if os.path.exists(file_path):\n",
    "        model = load_model(file_path)\n",
    "    else:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(19, activation='relu')) # Inputs\n",
    "\n",
    "        model.add(Dense(304, activation='relu')) # First hidden layer\n",
    "        model.add(Dropout(0.2))\n",
    "        # model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dense(304, activation='relu')) # Second hidden layer\n",
    "        model.add(Dropout(0.2))\n",
    "        # model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dense(304, activation='relu')) # Third hidden layer\n",
    "        model.add(Dropout(0.2))\n",
    "        # model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dense(304, activation='relu')) # Forth hidden layer\n",
    "        model.add(Dropout(0.2))\n",
    "        # model.add(BatchNormalization())\n",
    "\n",
    "        model.add(Dense(1, activation='relu')) # Output layer (regression)\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100)\n",
    "    model.fit(x=X_train, y=y_train, epochs=1000, batch_size=32,\n",
    "              validation_data=(X_val, y_val), verbose=0, callbacks=[early_stop])\n",
    "    model.save(file_path)\n",
    "\n",
    "def train_the_model(file_dir, model_name):\n",
    "    file_path = os.path.join(file_dir, 'processed', model_name + \".pkl\")\n",
    "    if os.path.exists(file_path):\n",
    "        # shuffle the data\n",
    "        df = pd.read_pickle(file_path).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    else:\n",
    "        raise ValueError(f\"{file_path} does not exist\")\n",
    "    \n",
    "    X = df.drop('Sale_Price', axis=1).values\n",
    "    y = df['Sale_Price'].values\n",
    "\n",
    "    # perform a 60% / 20% / 20% split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "    scale_and_fit(file_dir, model_name, X_train, X_test, y_train, y_test)\n",
    "    scale_and_fit(file_dir, 'all-county', X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c6159",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c83b346-54c7-49e5-93ee-d3a7f26bcec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipynb\n",
    "# !pip install ipywidgets\n",
    "# !pip install ipycanvas\n",
    "# !pip install ipyevents\n",
    "# !pip install watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed02a5e-c4ec-4e6d-8090-2f43d9aaaf49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ipywidgets: 8.1.6\n",
      "ipycanvas : 0.13.3\n",
      "ipyevents : 2.0.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -p ipywidgets,ipycanvas,ipyevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d67aa043-caf0-4bb1-9b5f-3efe88d39f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import ipycanvas\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0516ce8c-f32d-4c7b-8f92-8fc80083a2f3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c83d37fa1f2417a8fa0626eaed0d250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Output(),)), HBox(children=(Button(button_style='warning', description='pause', â€¦"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "\n",
    "pause_button = widgets.Button(\n",
    "    description = 'pause',\n",
    "    disabled = False,\n",
    "    button_style = 'warning',\n",
    "    tooltip = 'pause training the model',\n",
    "    icon = ''\n",
    ")\n",
    "\n",
    "continue_button = widgets.Button(\n",
    "    description = 'continue',\n",
    "    disabled = False,\n",
    "    button_style = 'success',\n",
    "    tooltip = 'continue training the model',\n",
    "    icon = ''\n",
    ")\n",
    "\n",
    "file_path = ''\n",
    "paused = True\n",
    "started = False\n",
    "counter = 0\n",
    "\n",
    "output = widgets.Output()\n",
    "def refreshOutput():\n",
    "    global counter\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        if not started:\n",
    "            display(HTML(\"<font size=\\\"5\\\" color=\\\"blue\\\">not started</font><br/>\"))\n",
    "        elif file_path == None:\n",
    "            display(HTML(\"<font size=\\\"5\\\" color=\\\"orange\\\">DONE</font><br/>\"))\n",
    "        elif paused:\n",
    "            display(HTML(\"<font size=\\\"5\\\" color=\\\"red\\\">paused!!</font><br/>\"))\n",
    "        else:\n",
    "            display(HTML(f\"<font size=\\\"5\\\" color=\\\"green\\\">loading {file_path}...</font><br/>\"))\n",
    "        display(HTML(f\"<font size=\\\"5\\\" color=\\\"purple\\\">counter at {counter}</font><br/>\"))\n",
    "\n",
    "header = widgets.Output()\n",
    "with header:\n",
    "    display(HTML(\"<h1>Let's Train the Model</h1?\"))\n",
    "    display(HTML(\"<font size=\\\"3\\\">hit pause or continue to control the iterations</font><br/>\"))\n",
    "    refreshOutput()\n",
    "\n",
    "def remove_files_matching(directory, extension):\n",
    "    files = glob.glob(os.path.join(directory, f\"*.{extension}\"))\n",
    "    for file_path in files:\n",
    "        os.remove(file_path)\n",
    "\n",
    "def pause_processing(button):\n",
    "    global paused\n",
    "    paused = True\n",
    "    refreshOutput()\n",
    "    \n",
    "def continue_processing(button):\n",
    "    global paused, counter\n",
    "    paused = False\n",
    "    if not thr.is_alive():\n",
    "        counter = 0\n",
    "        thr.start()\n",
    "    refreshOutput()\n",
    "\n",
    "pause_button.on_click(pause_processing)\n",
    "continue_button.on_click(continue_processing)\n",
    "\n",
    "def load():\n",
    "    global file_path, counter\n",
    "    \n",
    "    all_files = os.listdir(source_directory)\n",
    "    json_files = [f for f in all_files if f.endswith('.json')]\n",
    "    if not json_files:\n",
    "        raise ValueError(f\"No JSON files found in directory: {source_directory}\")\n",
    "    \n",
    "    for file_name in json_files:\n",
    "        while paused:\n",
    "            time.sleep(1)\n",
    "        model_name = os.path.splitext(file_name)[0]\n",
    "        file_path = os.path.join(file_directory, 'model', model_name + \".h5\")\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "        file_path = os.path.join(source_directory, file_name)\n",
    "        refreshOutput()\n",
    "        shutil.copy(file_path, file_directory)\n",
    "        preprocess_data(file_directory, model_name)\n",
    "        train_the_model(file_directory, model_name)\n",
    "        remove_files_matching(file_directory, \"json\")\n",
    "        counter += 1\n",
    "    file_path = None\n",
    "    refreshOutput()\n",
    "\n",
    "thr = threading.Thread(target=load)\n",
    "started = True\n",
    "\n",
    "### USER INTERFACE\n",
    "header_box = widgets.HBox([header])\n",
    "button_box = widgets.HBox([pause_button, continue_button])\n",
    "output_box = widgets.HBox([output])\n",
    "widgets.VBox([header_box, button_box, output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aab139",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
