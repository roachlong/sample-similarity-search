{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5be539ac-50f3-4f5a-b378-087aab8f7375",
   "metadata": {},
   "source": [
    "# Testing Real Estate Predictions\n",
    "The purpose of this notebook is to load a trained AI model for NJ real estate and use it to generate predictions agianst a pre-processed data file which will be compared against known property values to access the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643f6a0a-5716-44e5-aa1d-cd215927ac17",
   "metadata": {},
   "source": [
    "# 1 Load and Scale Sample Data\n",
    "First we'll load a sample pre-processed property file from our workspace and use the same scaler from our model creation to produce a consistent normalized view of the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2761d-4ae1-4ec8-b0cb-3c022620cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef6a7ac-2146-49c7-82eb-9fd09564bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "file_dir = 'data/sample'\n",
    "model_name = 'all-county'\n",
    "processed_file = '0204'\n",
    "\n",
    "file_path = os.path.join(file_dir, 'processed', processed_file + \".pkl\")\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_pickle(file_path).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    raise ValueError(f\"{file_path} does not exist\")\n",
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156618fe-1b91-4398-af27-c7528fa48855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "X = df.drop('Sale_Price', axis=1).values\n",
    "y = df['Sale_Price'].values\n",
    "    \n",
    "file_path = os.path.join(file_dir, 'scaler', processed_file + \".save\")\n",
    "if os.path.exists(file_path):\n",
    "    scaler = joblib.load(file_path)\n",
    "    X_specific = scaler.transform(X)\n",
    "else:\n",
    "    raise ValueError(f\"scaler for the model does not exist at {file_path}\")\n",
    "\n",
    "file_path = os.path.join(file_dir, 'scaler', model_name + \".save\")\n",
    "if os.path.exists(file_path):\n",
    "    scaler = joblib.load(file_path)\n",
    "    X = scaler.transform(X)\n",
    "else:\n",
    "    raise ValueError(f\"scaler for the model does not exist at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de20b33-2e36-4851-ab42-ae3178f43698",
   "metadata": {},
   "source": [
    "## 2 Generate Predictions\n",
    "Now we can load the pre-trained model and use the input data to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72404562-6163-4e40-b9b2-03ebe6101b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "file_path = os.path.join(file_dir, 'model', processed_file + \".h5\")\n",
    "if os.path.exists(file_path):\n",
    "    model_specific = load_model(file_path, custom_objects={'mse': MeanSquaredError()})\n",
    "else:\n",
    "    raise ValueError(f\"model does not exist at {file_path}\")\n",
    "\n",
    "file_path = os.path.join(file_dir, 'model', model_name + \".h5\")\n",
    "if os.path.exists(file_path):\n",
    "    model = load_model(file_path)\n",
    "else:\n",
    "    raise ValueError(f\"model does not exist at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8580957-707c-47a8-abb5-6568e0c2bfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score\n",
    "\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89575de-9ec7-4983-9f6a-42735f1d7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_specific = model_specific.predict(X_specific)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "print(f\"Specific  : {mean_absolute_error(y, predictions_specific)}\")\n",
    "print(f\"All County: {mean_absolute_error(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5b2c16-9dcd-4d97-a8e4-03646456bef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Specific  : {np.sqrt(mean_absolute_error(y, predictions_specific))}\")\n",
    "print(f\"All County: {np.sqrt(mean_absolute_error(y, predictions))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a640eb0b-dfab-4f17-af97-1f194fdd8925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Specific  : {explained_variance_score(y, predictions_specific)}\")\n",
    "print(f\"All County: {explained_variance_score(y, predictions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38dbc9b-0769-43dc-bf42-17603bd27d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_df = pd.DataFrame(y, columns=['value'])\n",
    "specific_df = pd.DataFrame(predictions_specific, columns=['prediction'])\n",
    "specific_df['value'] = y\n",
    "predictions_df = pd.DataFrame(predictions, columns=['prediction'])\n",
    "predictions_df['value'] = y\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.scatterplot(x='value', y='prediction', data=specific_df, ax=axes[0])\n",
    "sns.lineplot(x='value', y='value', data=y_df, ax=axes[0], color='r')\n",
    "axes[0].set_title(processed_file)\n",
    "\n",
    "sns.scatterplot(x='value', y='prediction', data=predictions_df, ax=axes[1])\n",
    "sns.lineplot(x='value', y='value', data=y_df, ax=axes[1], color='r')\n",
    "axes[1].set_title(model_name)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83cdd8-5d4b-493f-ba7f-bc59b8dcbccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_specific = y.reshape(X_specific.shape[0], 1) - predictions_specific\n",
    "errors = y.reshape(X.shape[0], 1) - predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a2386-0b04-4fa0-9771-95bc67b5e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.histplot(errors_specific, kde=True, stat='density', alpha=0.4, edgecolor=(1,1,1,0.4), ax=axes[0])\n",
    "axes[0].set_title(processed_file)\n",
    "axes[0].set_xlim(-300000, 300000)\n",
    "\n",
    "sns.histplot(errors, kde=True, stat='density', alpha=0.4, edgecolor=(1,1,1,0.4), ax=axes[1])\n",
    "axes[1].set_title(model_name)\n",
    "axes[1].set_xlim(-300000, 300000)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e08ab1-e421-4c6c-8a8c-f0a0e1949836",
   "metadata": {},
   "source": [
    "## 3 Use Similarity Scores\n",
    "Blind predictions have a lot of variablity, but what if we find similar properties and imput information from them to augment our input before generating the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23a2889-344c-40c5-99aa-5f36865c7272",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "array1 = np.array([1, 1, 1, 1])\n",
    "array2 = np.array([1, 1, 1, 0])\n",
    "similarity = cosine_similarity(array1, array2)\n",
    "print(f\"Cosine similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a896f37-4a94-4352-9164-e9c32d9484ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a74db-7e0e-4257-820b-702258e0b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_similar_vectors(query_vector, vector_list, top_n=5):\n",
    "    similarity_scores = np.array([cosine_similarity(query_vector, vector) for vector in vector_list])\n",
    "    top_indices = np.argsort(similarity_scores)[::-1][:top_n]\n",
    "    top_vectors = vector_list[top_indices]\n",
    "    top_similarity_scores = similarity_scores[top_indices]\n",
    "    return top_vectors, top_similarity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d181fe1-c62b-4eaa-bc1f-2ff07d29600b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(101)\n",
    "rand_idx = random.randint(0, len(df))\n",
    "\n",
    "example = X[rand_idx]\n",
    "example_pred = model.predict(example.reshape(1, -1))\n",
    "\n",
    "top_vectors, top_similarity_scores = top_similar_vectors(example, X)\n",
    "column_names = df.columns.values\n",
    "column_names = column_names[column_names != 'Sale_Price']\n",
    "column_names\n",
    "\n",
    "similar_agg = pd.DataFrame(top_vectors, columns=column_names).mean().values\n",
    "similar_pred = model.predict(similar_agg.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b7edc7-6cff-43c9-9ced-b2a9ad18d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"actual: {df.iloc[rand_idx]['Sale_Price']}, example: {example_pred[0][0]}, similar: {similar_pred[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f1b56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d5cb7c5-b646-4b9c-b531-60153e6dbd03",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.1 Calculated Taxes and Year\n",
    "This information is embedded in the recorded taxes and is not needed, therefore we will drop the columns from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989d899e-fbc6-4996-94c0-a5976100fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Calculated_Taxes', 'Calculated_Taxes_Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9516e1-7235-4908-9c0b-956f3a42dc56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.2 County\n",
    "We'll convert this into a category and then transpose the data into dummy columns, essentially a bitmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753f8b7-091f-4577-81ed-ba0c58a772da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['County'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcd1083-dcf3-459a-a486-3c33d202f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['County'] = df['County'].map({\n",
    "    1: 'Atlantic',\n",
    "    2: 'Bergen',\n",
    "    3: 'Burlignton',\n",
    "    4: 'Camden',\n",
    "    5: 'Cape May',\n",
    "    6: 'Cumberland',\n",
    "    7: 'Essex',\n",
    "    8: 'Gloucester',\n",
    "    9: 'Hudson',\n",
    "    10: 'Hunterdon',\n",
    "    11: 'Mercer',\n",
    "    12: 'Middlesex',\n",
    "    13: 'Monmouth',\n",
    "    14: 'Morris',\n",
    "    15: 'Ocean',\n",
    "    16: 'Passaic',\n",
    "    17: 'Salem',\n",
    "    18: 'Somerset',\n",
    "    19: 'Sussex',\n",
    "    20: 'Union',\n",
    "    21: 'Warren'\n",
    "})\n",
    "df['County'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b9bd0-c2ca-4606-aae3-b5497fd76c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(df['County'], drop_first=True)\n",
    "df = df.drop('County', axis=1)\n",
    "# in case sample data is missing a county, we'll add\n",
    "# the columns manually and then overwrite the values\n",
    "# skip Atlantic because it would have already been dropped\n",
    "counties = ['Bergen', 'Burlignton', 'Camden', 'Cape May',\n",
    "            'Cumberland', 'Essex', 'Gloucester', 'Hudson',\n",
    "            'Hunterdon', 'Mercer', 'Middlesex', 'Monmouth',\n",
    "            'Morris', 'Ocean', 'Passaic', 'Salem', 'Somerset',\n",
    "            'Sussex', 'Union', 'Warren']\n",
    "for county in counties:\n",
    "    if county in dummies.columns:\n",
    "        df[county] = dummies[county]\n",
    "    else:\n",
    "        df[county] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b5217-ba7c-45d2-8f9d-11cfdb6c5232",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.3 NU Code\n",
    "There's too much uncertainty around property values where an NU Code is applied, so we'll drop those records and remove this column from our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484532b-3f1b-4b62-af79-4fd5a1f35973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['NU_Code'] == 99]\n",
    "df = df.drop('NU_Code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc07460d-4638-4bbd-9be3-0339bcba8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['NU_Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79269a44-9a01-40fe-9ac2-d235a7d04ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['NU_Code'] = df['NU_Code'].map({\n",
    "#     99: 'None',\n",
    "#     -1: 'Unknown',\n",
    "#     0: 'Unknown',\n",
    "#     1: 'immediate family',\n",
    "#     2: 'love and affection',\n",
    "#     3: 'corporation',\n",
    "#     4: 'convenience',\n",
    "#     5: 'transfer',\n",
    "#     6: 'apportionment',\n",
    "#     7: 'subsequent to assessment',\n",
    "#     8: 'undivided interest',\n",
    "#     9: 'governmental lien',\n",
    "#     10: 'trustees',\n",
    "#     11: 'judicial',\n",
    "#     12: 'sheriff',\n",
    "#     13: 'benefit of creditors',\n",
    "#     14: 'doubtful title',\n",
    "#     15: 'political',\n",
    "#     16: 'more than one taxing district',\n",
    "#     17: 'charitable',\n",
    "#     18: 'foreclosure',\n",
    "#     19: 'physical damage',\n",
    "#     20: 'right-of-way',\n",
    "#     21: 'affordable housing',\n",
    "#     22: 'exchange',\n",
    "#     23: 'industrial',\n",
    "#     24: 'influenced',\n",
    "#     25: 'realty transfer fee act',\n",
    "#     26: 'not compelled',\n",
    "#     27: 'reassessment',\n",
    "#     28: 'leaseback',\n",
    "#     29: 'freeze act',\n",
    "#     30: 'package deal',\n",
    "#     31: 'federal or state',\n",
    "#     32: 'building omitted',\n",
    "#     33: 'exempt property'\n",
    "# })\n",
    "# df['NU_Code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306fc6d-b065-4fdc-bb4f-56aceb5daa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummies = pd.get_dummies(df['NU_Code'], drop_first=True)\n",
    "# df = df.drop('NU_Code', axis=1)\n",
    "# df = pd.concat([df, dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2712b59-4aa9-48ae-8897-5c762fe8c568",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.4 Property Class\n",
    "Since our model is only for residential properties we can remove this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca719aa-20a4-4652-8e42-da22c0ac89d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Property_Class', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee7a2c9-384f-4306-8adc-ec207781e752",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.5 Ratio Year / Recorded Taxes Year\n",
    "This information doesn't vary between records and doesn't have a very strong correlation with sales price, therefore we can drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f85140-ea20-41cf-ab8b-f632973f2a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['RatioYear', 'Recorded_Taxes_Year'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc9df18-0cd5-4383-906e-d4affb873212",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.6 Total Units\n",
    "This data seems to have very little impact on sales price for residential homes and therefore we will drop the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299a4fd-8c4b-4c63-86d7-889899896e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('TotalUnits', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45524795-1694-4e09-b47e-f9b69c81fa8b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.7 Year 1 / Year 2\n",
    "These should really be int values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fa516c-d6ab-4424-a0a3-0f1638f3e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Year_1'] = df['Year_1'].astype(int)\n",
    "df['Year_2'] = df['Year_2'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314c10ca-e8ac-4e1f-aa21-68b67243aee6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4.8 Year 2 Assessments\n",
    "This information is closely related to year 1 assessments and therefore we will ignore it for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f772d4d-8af9-41e2-a666-30b7a0616469",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Year_2', 'Land_Assmnt_2', 'Building_Assmnt_2', 'Total_Assmnt_2'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eebd5b-b962-430f-884a-a4ec3676428f",
   "metadata": {},
   "source": [
    "## 4.9 Review and Save the Dataframe\n",
    "Now we can store our data set that will be used to train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6306a7c-8662-4446-9c38-e1f158cb18c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f72eed1-b036-41d4-9182-dbe039f1bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('data/sample/processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b6242-d20d-49b2-8d0b-0090f8676870",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_pickle('data/sample/processed.pkl')\n",
    "print(test.shape)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b28c7f-80ed-43db-8bee-76f19e757585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-macos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
